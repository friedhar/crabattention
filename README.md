# Crabattention - Multi-Flavour Attention Implementations In Rust

Attention algorithms are the core of modern generative models, including, but not limited to - transformers.

The Library provides optimized implementations of various Attention Algorithms, with the goal of supporting multiple backends.

Currently, the only supported target device is plain ol' CPU LLIR. 




Crabattention is a WIP. contributions are welcome.

