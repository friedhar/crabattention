# Crabattention - Multi-Flavour Attention Implementations In Rust

Attention algorithms are the core of modern generative models, including, but not limited to - transformers.
The Library provides optimized implementations of various Attention Algorithms, with the goal of supporting multiple backends.
Currently, there is support just for (albeit pipelined) CPU implementations. 




Crabattention is a WIP. contributions are welcome.

