# Crabattention - Multi-Flavour Attention Implementations In Rust

Attention algorithms are the core of modern generative models, including, but not limited to - transformers.

The Library provides optimized implementations of various Attention Algorithms, with the goal of supporting multiple backends.

Currently, the only supported target device is plain ol' CPU LLIR. 

## (Semi-Scientific) Benchmarks
> NOTE: The benchmarks we're done on a standard work machine, a M4 Macbok Pro. varying hardware obviously has high beta to the results.


-----
Crabattention is a WIP. contributions are welcome.

